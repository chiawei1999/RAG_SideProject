import os
from dotenv import dotenv_values
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from uuid import uuid4

config = dotenv_values(".env")
QDRANT_URL = config["QDRANT_URL"] # Qdrant ä¼ºæœå™¨çš„ URL
COLLECTION_NAME = config["COLLECTION_NAME"] # Qdrant é›†åˆåç¨±
EMBEDDING_DIM = int(config.get("EMBEDDING_DIM", 384)) # åµŒå…¥å‘é‡çš„ç¶­åº¦

def parse_text_blocks(text: str) -> list[dict]:
    """
    è§£ææ¨™è¨˜æœ‰ ã€type: xxxã€‘ çš„æ–‡æœ¬å€å¡Š
    
    Args:
        text: åŒ…å«å¤šå€‹æ¨™è¨˜å€å¡Šçš„æ–‡æœ¬
        
    Returns:
        list: åŒ…å«æ¯å€‹å€å¡Šé¡å‹å’Œå…§å®¹çš„å­—å…¸åˆ—è¡¨
    """
    blocks = []
    # ä½¿ç”¨ ã€type: åˆ†å‰²æ–‡æœ¬
    sections = text.split("ã€type:")
    
    # ç¬¬ä¸€å€‹åˆ†å‰²å¯èƒ½æ˜¯ç©ºçš„ï¼ˆå¦‚æœæ–‡æœ¬ä»¥ã€type:é–‹é ­ï¼‰
    sections = [s for s in sections if s.strip()]
    
    for section in sections:
        # æå–é¡å‹å’Œå…§å®¹
        try:
            # åˆ†å‰²å‡ºé¡å‹å’Œå…§å®¹
            type_end = section.find("ã€‘")
            if type_end == -1:
                continue
                
            block_type = section[:type_end].strip()
            content = section[type_end+1:].strip()
            
            blocks.append({
                "type": block_type,
                "content": content
            })
        except Exception as e:
            print(f"è§£æå€å¡Šæ™‚å‡ºéŒ¯: {e}")
            continue
            
    return blocks

def split_text(text: str, chunk_size=500, overlap=100):
    """
    æ–‡æœ¬åˆ‡å‰²ï¼šæ‰‹åˆ»ç‰ˆæœ¬ï¼ˆæ¨¡æ“¬ RecursiveCharacterTextSplitterï¼‰
    """
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


def load_clean_text(path: str) -> str:
    """
    è®€å–æ¸…ç†å¾Œçš„ txt
    """
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def vectorize_and_store(text_path: str):
    """
    ä¸»æµç¨‹ï¼šåˆ‡å‰² âœ åµŒå…¥ âœ å¯«å…¥ Qdrant
    """

    # åˆå§‹åŒ– Qdrant
    client = QdrantClient(
    url=QDRANT_URL,
    api_key=config["QDRANT_API_KEY"]  # æ–°å¢é€™ä¸€è¡Œï¼Œè¨˜å¾—åœ¨ .env è£œä¸Š
    )
    collections = client.get_collections().collections
    exists = any(c.name == COLLECTION_NAME for c in collections)

    if not exists:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)
        )
        print(f"âœ… å·²å»ºç«‹é›†åˆï¼š{COLLECTION_NAME}")
    else:
        print(f"ğŸ“¦ ä½¿ç”¨æ—¢æœ‰é›†åˆï¼š{COLLECTION_NAME}")

    # è¼‰å…¥æ–‡æœ¬
    raw_text = load_clean_text(text_path)
    
    # è§£ææ–‡æœ¬å€å¡Š
    blocks = parse_text_blocks(raw_text)
    
    # å‘é‡åŒ–æ¨¡å‹
    model = SentenceTransformer("jinaai/jina-embeddings-v2-base-zh")
    
    # æº–å‚™æ‰€æœ‰çš„ chunks å’Œå°æ‡‰çš„ metadata
    all_chunks = []
    all_metadata = []
    
    for i, block in enumerate(blocks):
        block_chunks = split_text(block["content"])
        for chunk in block_chunks:
            all_chunks.append(chunk)
            all_metadata.append({
                "type": block["type"],
                "block_id": i,  # æ–°å¢æ¬„ä½
                "text": chunk
            })

    
    print(f"ğŸ“„ å…±è§£æå‡º {len(blocks)} å€‹å€å¡Šï¼Œç”¢ç”Ÿ {len(all_chunks)} å€‹ chunks")
    
    # å‘é‡åŒ–æ‰€æœ‰ chunks
    embeddings = model.encode(all_chunks, normalize_embeddings=True)
    
    # çµ„æˆ Qdrant Point çµæ§‹
    points = [
        PointStruct(
            id=str(uuid4()),
            vector=embeddings[i],
            payload=all_metadata[i]
        )
        for i in range(len(all_chunks))
    ]

    # å¯«å…¥è³‡æ–™åº«
    client.upsert(collection_name=COLLECTION_NAME, points=points)
    print(f"ğŸ” å…±ä¸Šå‚³ {len(points)} ç­† chunk è‡³ Qdrant")


if __name__ == "__main__":
    vectorize_and_store("./data/clean_text.txt")